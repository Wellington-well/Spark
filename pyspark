# https://spark.apache.org/docs/latest/api/python/getting_started/install.html
# https://spark.apache.org/docs/latest/sql-data-sources-csv.html

# importando pyspark( SQL )
from pyspark.sql import SparkSession

# criando sessão pyspark
spark = SparkSession.builder.getOrCreate()

# importando novas bibliotecas para criação do dataframe
from datetime import datetime, date
from pyspark.sql import Row

# criando um dataframe
df = spark.createDataFrame([
    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))
])
# df.show() ou df ( df.show() = df.head())

# vendo as colunas do DF
df.columns

# selecionando as colunas e passando o describe dos dados | acredito que de para usar sem describe()
df.select("a", "b", "c").describe().show()

# para visualizar o DF como se fosse no pandas
df.toPandas()

# criando uma nova coluna ( df.c(pegar a coluna "c" e aplicando o upper(deixar maiusculo))
df.withColumn('upper_c', upper(df.c)).show()

# Passando um filtro para a coluna ( df.a==1 ( quando a coluna "a" for igual 1 ) )
df.filter(df.a == 1).show()

----------------------------------------------------------------------

# criando DF
df = spark.createDataFrame([
    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],
    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],
    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])
df.show()

# passando um groupby
df.groupby('color').avg().show()

# Formas de leitura de arquivo
df.write.csv('Caminho_do_arquivo.csv', header=True)
spark.read.csv('Caminho_do_arquivo.csv', header=True)
